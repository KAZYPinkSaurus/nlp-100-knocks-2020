{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章: 機械学習\n",
    "本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50. データの入手・整形\n",
    "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "3. 抽出された事例をランダムに並び替える．\n",
    "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2pageSessions.csv check.txt         newsCorpora.csv   readme.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls files/NewsAggregatorDataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: Error reading files\n",
      "210713\t\"The Best Reactions To The Supposed Video of Solange Knowles & Jay Z  ...\thttp://www.hiphopdx.com/index/news/id.28728/title.-the-best-reactions-to-the-supposed-video-of-solange-knowles-jay-z-fighting-in-an-elevator-list-by-time/\tHipHopDX\te\tdku0uRoeehpC9JM1RoZ4n0fg8cyoM\twww.hiphopdx.com\t1399983366398\n"
     ]
    }
   ],
   "source": [
    "!head -210713 files files/NewsAggregatorDataset/newsCorpora.csv | tail -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Best Reactions To The Supposed Video of Solange Knowles & Jay Z  ...\n",
    "## ↑ダブルクオーテーションが片側しかないとDictreaderがうまく読み込まないので修正する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "PUBLISHERS = ('Reuters','Huffington Post','Businessweek','Contactmusic.com','Daily Mail')\n",
    "CATEGORIES = ('b','t','e','m')\n",
    "CATEGORIES_DIC = {'b':0,'t':1,'e':2,'m':3}\n",
    "CATEGORIES_DIC_inv = {0:'business',1:'science and technology',2:'entertaiment',3:'health'}\n",
    "\n",
    "y_X_list = []\n",
    "\n",
    "with open('files/NewsAggregatorDataset/newsCorpora.csv') as f:\n",
    "    reader = csv.DictReader(f,delimiter='\\t',fieldnames=[\"id\",\"title\",\"url\",\"publisher\",\"category\",\"story\",\"hostname\",\"timestamp\"])\n",
    "    y_X_list = [(CATEGORIES_DIC[row['category']],row['title']) for row in reader if row['publisher'] in PUBLISHERS and row['category'] in CATEGORIES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y len :13340, X len: 13340\n",
      "train:10672, valid:1334, test:1334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List\n",
    "import csv\n",
    "y = [y for y, x in  y_X_list]\n",
    "X = [x for y, x in  y_X_list]\n",
    "\n",
    "\n",
    "print(f\"y len :{len(y)}, X len: {len(X)}\")\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"train:{len(X_train)}, valid:{len(X_valid)}, test:{len(X_test)}\")\n",
    "\n",
    "train = [[_y_train,_x_train] for _y_train,_x_train  in zip(y_train,X_train)]\n",
    "valid = [[_y_valid,_x_valid] for _y_valid,_x_valid in zip(y_valid,X_valid)]\n",
    "test = [[_y_test,_x_test] for _y_test,_x_test in zip(y_test,X_test)]\n",
    "\n",
    "def save_data(data:List[List],file_name):\n",
    "    with open(file_name, mode='w', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f,delimiter='\\t')\n",
    "        writer.writerows(data)\n",
    "\n",
    "save_data(train,'files/train.txt')\n",
    "save_data(valid,'files/valid.txt')\n",
    "save_data(test,'files/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tDraghi Unites Euro Bulls With Bears Watching $1.35: Currencies\r",
      "\r\n",
      "3\tA Guide To Spring Gardening, For Allergy-Sufferers\r",
      "\r\n",
      "2\tBy Odin's beard: Marvel creates a storm of controversy as it reveals Thor is a  ...\r",
      "\r\n",
      "0\tWalmart Strikes Deal That Will Hopefully Make Organic Food Cheaper\r",
      "\r\n",
      "2\tThe Voice contestant Kristen Merlin goes silent mid-song after microphone fails  ...\r",
      "\r\n",
      "1\tGender Non-Conforming Teen Forced To Remove Makeup For Driver's License  ...\r",
      "\r\n",
      "2\tFrom Cleaning Toilets to Intergalactic Royalty: Jupiter Ascending [Trailer +  ...\r",
      "\r\n",
      "1\tCORRECTED-Sprint's revenue beats estimate as network upgrade progresses\r",
      "\r\n",
      "2\t'My mother touched me I'm certain': Hip hop mogul gunned down at mom's  ...\r",
      "\r\n",
      "0\tFairfax CEO Watsa Probed by Regulator in Trading Review\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head files/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538 0\n",
      "1205 1\n",
      "4228 2\n",
      " 701 3\n"
     ]
    }
   ],
   "source": [
    "!cut -f 1 files/train.txt | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 531 0\r\n",
      " 155 1\r\n",
      " 529 2\r\n",
      " 119 3\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f 1 files/valid.txt | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 558 0\r\n",
      " 164 1\r\n",
      " 522 2\r\n",
      "  90 3\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f 1 files/test.txt | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51. 特徴量抽出\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量1:Bag of Words (1-gram, 2-gram, binary=True)\n",
    "参考:https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (10672, 75528)\n",
      "valid: (1334, 75528)\n",
      "test: (1334, 75528)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "count_vect = CountVectorizer(binary=True,ngram_range=(1,2),dtype=np.int8)\n",
    "X_train_bow = count_vect.fit_transform(X_train).toarray()\n",
    "X_valid_bow = count_vect.transform(X_valid).toarray()\n",
    "X_test_bow = count_vect.transform(X_test).toarray()\n",
    "\n",
    "print(f\"train: {X_train_bow.shape}\\nvalid: {X_valid_bow.shape}\\ntest: {X_test_bow.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10672, 1)\n",
      "type :int8 , shape:(10672, 75528)\n",
      "type : int8\n",
      "train: (10672, 75529)\n",
      "valid: (1334, 75529)\n",
      "test: (1334, 75529)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.array([y_train],dtype=np.int8).T.shape)\n",
    "print(f\"type :{X_train_bow.dtype} , shape:{X_train_bow.shape}\")\n",
    "train_bow = np.hstack((np.array([y_train]).T, X_train_bow)).astype(np.int8)\n",
    "valid_bow = np.hstack((np.array([y_valid]).T, X_valid_bow)).astype(np.int8)\n",
    "test_bow = np.hstack((np.array([y_test]).T, X_test_bow)).astype(np.int8)\n",
    "print(f\"type : {train_bow.dtype}\")\n",
    "print(f\"train: {train_bow.shape}\\nvalid: {valid_bow.shape}\\ntest: {test_bow.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存すると各ファイル5GB以上になるので保存しない!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量2:tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量3: word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 52. 学習\n",
    "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0,dual=True,solver='liblinear',verbose=1).fit(X_train_bow,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 53. 予測\n",
    "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entertaiment'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense = \"science is important to improve your life.\"\n",
    "CATEGORIES_DIC_inv[clf.predict(count_vect.transform([sentense]).toarray())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 54. 正解率の計測\n",
    "52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_train_pred =  clf.predict(X_train_bow)\n",
    "y_test_pred = clf.predict(X_test_bow)\n",
    "\n",
    "print(f\"train accuracy:{accuracy_score(y_train,y_train_pred)}\\ntest accuracy{accuracy_score(y_test,y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 55. 混同行列の作成\n",
    "52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(f\"train accuracy:\\n{confusion_matrix(y_train,y_train_pred)}\\ntest accuracy:\\n{confusion_matrix(y_test,y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 56. 適合率，再現率，F1スコアの計測\n",
    "52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "pre_micro =precision_score(y_test,y_test_pred,average='micro')\n",
    "pre_macro =precision_score(y_test,y_test_pred,average='macro')\n",
    "rec_micro =recall_score(y_test,y_test_pred,average='micro')\n",
    "rec_macro = recall_score(y_test,y_test_pred,average='macro')\n",
    "f1_micro =f1_score(y_test,y_test_pred,average='micro')\n",
    "f1_macro =f1_score(y_test,y_test_pred,average='macro')\n",
    "print(\"precision micro:\",precision_score(y_test,y_test_pred,average='micro'))\n",
    "print(\"precision macro:\",precision_score(y_test,y_test_pred,average='macro'))\n",
    "print(\"recall micro:\",recall_score(y_test,y_test_pred,average='micro'))\n",
    "print(\"recall macro:\",recall_score(y_test,y_test_pred,average='macro'))\n",
    "print(\"f1 micro:\",f1_score(y_test,y_test_pred,average='micro'))\n",
    "print(\"f1 macro:\",f1_score(y_test,y_test_pred,average='macro'))\n",
    "\n",
    "# ↓真面目に計算するとこんな感じ\n",
    "# print(\"f1 micro:\",2*pre_micro*rec_micro/(pre_micro+rec_micro))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 別々で計算すると....(イケてる書き方知りたい)\n",
    "a = np.array(y_test)[y_test_pred == 0]\n",
    "a[a > 1] = 1\n",
    "pre_0 = precision_score(1 - a , 1-y_test_pred[y_test_pred == 0])\n",
    "\n",
    "a = np.array(y_test)[y_test_pred == 1]\n",
    "a[a != 1] = 0\n",
    "pre_1 = precision_score(a , y_test_pred[y_test_pred == 1])\n",
    "\n",
    "a = np.array(y_test)[y_test_pred == 2]\n",
    "a[a != 2] = 0\n",
    "a[a == 2] = 1\n",
    "pre_2 = precision_score(a , y_test_pred[y_test_pred == 2]-1)\n",
    "\n",
    "a = np.array(y_test)[y_test_pred == 3]\n",
    "a[a != 3] = 0\n",
    "a[a == 3] = 1\n",
    "pre_3 = precision_score(a , y_test_pred[y_test_pred == 3]-2)\n",
    "print('precision micro:',(pre_0*len(y_test_pred[y_test_pred == 0])+pre_1*len(y_test_pred[y_test_pred == 1])+pre_2*len(y_test_pred[y_test_pred == 2])+pre_3*len(y_test_pred[y_test_pred == 3]))/len(y_test))\n",
    "print('precision macro:',(pre_0+pre_1+pre_2+pre_3)/4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 57. 特徴量の重みの確認\n",
    "52で学習したロジスティック回帰モデルの中で，重みの高い特徴量トップ10と，重みの低い特徴量トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = clf.coef_.argsort()[:,:10]\n",
    "bottom10 = clf.coef_.argsort()[:,-10:][:,::-1]\n",
    "print(CATEGORIES_DIC_inv[0],\" top 10:\",','.join(np.array(count_vect.get_feature_names())[top10[0]]))\n",
    "print(CATEGORIES_DIC_inv[0],\" bottom 10:\",','.join(np.array(count_vect.get_feature_names())[bottom10[0]]))\n",
    "\n",
    "print(CATEGORIES_DIC_inv[1],\" top 10:\",','.join(np.array(count_vect.get_feature_names())[top10[1]]))\n",
    "print(CATEGORIES_DIC_inv[1],\" bottom 10:\",','.join(np.array(count_vect.get_feature_names())[bottom10[1]]))\n",
    "\n",
    "print(CATEGORIES_DIC_inv[2],\" top 10:\",','.join(np.array(count_vect.get_feature_names())[top10[2]]))\n",
    "print(CATEGORIES_DIC_inv[2],\" bottom 10:\",','.join(np.array(count_vect.get_feature_names())[bottom10[2]]))\n",
    "\n",
    "print(CATEGORIES_DIC_inv[3],\" top 10:\",','.join(np.array(count_vect.get_feature_names())[top10[3]]))\n",
    "print(CATEGORIES_DIC_inv[3],\" bottom 10:\",','.join(np.array(count_vect.get_feature_names())[bottom10[3]]))\n",
    "# 見た感じどうやら値が低いほうが特徴を表しているっぽい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 58. 正則化パラメータの変更\n",
    "ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，学習時の過学習（overfitting）の度合いを制御できる．異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 59. ハイパーパラメータの探索\n",
    "学習アルゴリズムや学習パラメータを変えながら，カテゴリ分類モデルを学習せよ．検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ．また，その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('nlp-100-knocks-2020-UTXiV_Xd-py3.8': venv)",
   "language": "python",
   "name": "python38264bitnlp100knocks2020utxivxdpy38venv6f1bdc414e1c42079574c87f4d492959"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
